{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-19T16:41:57.268130Z",
     "start_time": "2024-03-19T16:41:57.258860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nKey concepts:\\n* KV-cache\\n* Grouped query attention\\n* RMS Norm\\n* SwiGLU\\n* Rotary Positional embedding\\n\\nNotes:\\n- Layer normalization is done primarily to deal with internal covariate shift\\n\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Key concepts:\n",
    "* KV-cache\n",
    "* Grouped query attention\n",
    "* RMS Norm\n",
    "* SwiGLU\n",
    "* Rotary Positional embedding\n",
    "\n",
    "Notes:\n",
    "- Layer normalization is done primarily to deal with `internal co-variate shift`, which is to avoid excessive changes in the distribution of the neuron's values due to drastic adjustments made by SGD. This slows down training.\n",
    "- LayerNorm: a unique (mean, variance) pair for each sample.\n",
    "- BatchNorm: a unique (mean, variance) pair for each feature.\n",
    "- RMSNorm: Hypothesizes that the scaling is mostly responsible for the success of the normalization. The re-centering is thus not needed and the mean doesn't have to be calculated.\n",
    "- Rotary positional embedding: Uses a slightly different representation (relative) for the analysis.\n",
    "    - They are parametrically efficient to compute.\n",
    "    - They show better invariance to permutations.\n",
    "    - Better generalization.\n",
    "    - Easy to implement.\n",
    "- Rotary positional embedding is only applied to the query and keys.\n",
    "- Rotary positional embedding is only applied after Q and K are multiplied by W.\n",
    "- KV-cache is an important concept that can be applied to all Transformer models (only during INFERENCE)\n",
    "- KV-cache caches the K and V values from previous steps. but our Q is only one vector. \n",
    "  Instead of doing a (N,F)x(N,F).T self-attention computation each time, we only do a \n",
    "  (1,F)x(N,F).T each time where (1,F) is the dimension of Q. This allows us to skip the re--calculation of the entire self-attention matrix each time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "540d452c0dff4943"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "Layer Normalization\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34039924ed6d2ff3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
