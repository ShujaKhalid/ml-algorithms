{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Key concepts:\n",
    "* RMS Norm\n",
    "* KV-cache\n",
    "* Rotary Positional embedding\n",
    "* Grouped query attention\n",
    "* SwiGLU\n",
    "\n",
    "Notes:\n",
    "- Layer normalization is done primarily to deal with `internal co-variate shift`, which is to avoid excessive changes in the distribution of the neuron's values due to drastic adjustments made by SGD. This slows down training.\n",
    "- LayerNorm: a unique (mean, variance) pair for each sample.\n",
    "- BatchNorm: a unique (mean, variance) pair for each feature.\n",
    "- RMSNorm: Hypothesizes that the scaling is mostly responsible for the success of the normalization. The re-centering is thus not needed and the mean doesn't have to be calculated.\n",
    "- Rotary positional embedding: Uses a slightly different representation (relative) for the analysis.\n",
    "    - They are parametrically efficient to compute.\n",
    "    - They show better invariance to permutations.\n",
    "    - Better generalization.\n",
    "    - Easy to implement.\n",
    "- Rotary positional embedding is only applied to the query and keys.\n",
    "- Rotary positional embedding is only applied after Q and K are multiplied by W.\n",
    "- KV-cache is an important concept that can be applied to all Transformer models (only during INFERENCE)\n",
    "- KV-cache caches the K and V values from previous steps. but our Q is only one vector. \n",
    "  Instead of doing a (N,F)x(N,F).T self-attention computation each time, we only do a \n",
    "  (1,F)x(N,F).T each time where (1,F) is the dimension of Q. This allows us to skip the re-calculation of the entire self-attention matrix each time.\n",
    "- In GPUs, memory transfer is really expensive and about 20x slower than matrix multiplication operations.\n",
    "- Total operations -> O(bnd**2)\n",
    "- Total memory accesses -> O(bnd + bhn**2 + d**2)\n",
    "- Memory access is not the botleneck here since (Total memory)/(Total operations) <<< 1\n",
    "- multi-query attention: When using group query attention, we only calculate the multi-heads on the query. N-heads per query for each key and value.\n",
    "- GROUPED multi-query attention: When using GMQA, we reduce the no. of heads for the K and V values but don't remove them completely. A good compromise between speed and quality.\n",
    "- SwiGLU = x * (1/(1+exp(-beta * x))) -> Works due to divine benevolence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "540d452c0dff4943"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "Layer Normalization\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34039924ed6d2ff3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
