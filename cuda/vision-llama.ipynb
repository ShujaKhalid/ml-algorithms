{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Main components of Vision-Llama\n",
    "\n",
    "- A generic MLP class\n",
    "- An MLP Transformer\n",
    "- Multi-head attention\n",
    "- Transformer Layer\n",
    "- Transformer\n",
    "- Transformer Mapper\n",
    "\n",
    "Note: Class structure and code logic adopted from the official repo for Vision-Llama\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Generic MLP Module with Tanh activation\n",
    "'''\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sizes = sizes\n",
    "        self.layers = []\n",
    "        for i in range(len(sizes)-1):\n",
    "            self.layers.append(nn.Linear(sizes[i], sizes[i+1], bias=bias))\n",
    "            if i < len(sizes)-2:\n",
    "                self.layers.append(act())\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9781b07a0387b20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Generic MLP based transformer\n",
    "'''\n",
    "\n",
    "class MLPTransformer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out, dropout_ratio, act=nnf.relu, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = nn.Linear(dim_in, dim_h)\n",
    "        self.fc2 = nn.Linear(dim_h, dim_out)\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a08dba54c824cb52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Generic Multi-head Attention Architecture\n",
    "'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, bias, dropout, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dim_self = dim_self\n",
    "        self.dim_ref = dim_ref\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.dim_self//self.num_heads\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.to_q = nn.Linear(self.dim_self, self.dim_self, bias=True)\n",
    "        self.to_k = nn.Linear(self.dim_ref, self.dim_self, bias=True) # dim_ref -> dim_self mapping\n",
    "        self.to_v = nn.Linear(self.dim_ref, self.dim_self, bias=True) # dim_ref -> dim_self mapping\n",
    "        self.project = nn.Linear(self.dim_self, self.dim_self, bias=True) # dim_ref -> dim_self mapping\n",
    "        \n",
    "    def forward(self, X, y=None, mask=None):\n",
    "        y = y if y is not None else X\n",
    "        b, n, c = X.shape\n",
    "        _, m, c = y.shape\n",
    "        q = self.to_q(X).view(b, n, self.heads, self.head_dim) # Add an additional `heads` dimension\n",
    "        k = self.to_k(y).view(b, m, self.heads, self.head_dim) # Add an additional `heads` dimension\n",
    "        v = self.to_v(y).view(b, m, self.heads, self.head_dim) # Add an additional `heads` dimension\n",
    "        attention = torch.einsum(\"bnhd,bmhd->bnmh\", q, k) * self.scale\n",
    "        \n",
    "        # Add causal mask to self.attention\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask.unsqueeze(1).unsqueeze(2), float(\"-inf\"))\n",
    "\n",
    "        # Attention module\n",
    "        attention = nnf.softmax(attention)\n",
    "\n",
    "        # Taking the softmax of the mask filled elements yields a probability of 0\n",
    "        out = torch.einsum(\"bnmh,bmhd->bnhd\", attention, v) # resurgence (dim-wise -> back to square one)\n",
    "        out = out.view(b, n, c)\n",
    "        \n",
    "        # One last linear layer\n",
    "        out = self.project(out)\n",
    "        \n",
    "        return out, attention\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "938110c6756d3e5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\left( \\frac{x - \\mu}{\\sigma + \\epsilon} \\right) + \\beta\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2705bc41eb414c1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "gamma: scale parameter (scaling according to the importance of features)\n",
    "beta: shift parameter (allows it to find the optimal mean values)\n",
    "'''\n",
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, features, epsilon=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(features))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(features))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d02f9fdac5a75deb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Transformer Layer\n",
    "'''\n",
    "class Transformer(nn.Module):\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        attentions = []\n",
    "        for layer in self.layers:\n",
    "            x, att = layer.forward_with_attention(x, y, mask)\n",
    "            attentions.append(att)\n",
    "        return x, attentions\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i % 2 == 0 and self.enc_dec: # cross\n",
    "                x = layer(x, y)\n",
    "            elif self.enc_dec:  # self\n",
    "                x = layer(x, x, mask)\n",
    "            else:  # self or cross\n",
    "                x = layer(x, y, mask)\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n",
    "                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n",
    "        super(Transformer, self).__init__()\n",
    "        dim_ref = dim_ref if dim_ref is not None else dim_self\n",
    "        self.enc_dec = enc_dec\n",
    "        if enc_dec:\n",
    "            num_layers = num_layers * 2\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0 and enc_dec:  # cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            elif enc_dec:  # self\n",
    "                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            else:  # self or cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "        self.layers = nn.ModuleList(layers)    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "562770efc1070498"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
    "        x = x + x_\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attention\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
    "                 norm_layer: nn.Module = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim_self)\n",
    "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
    "        self.norm2 = norm_layer(dim_self)\n",
    "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba282f5b304b9119"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
