{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:49:30.011697Z",
     "start_time": "2024-02-27T20:49:30.005952Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3784184826.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 4\u001B[0;36m\u001B[0m\n\u001B[0;31m    Important functions:\u001B[0m\n\u001B[0m              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here we go through the Imagen modules\n",
    "\n",
    "Important functions:\n",
    "- GaussianDiffusionContinuousTimes\n",
    "- t5_encode_text\n",
    "- p_mean_variance\n",
    "- p_sample\n",
    "- p_sample_loop\n",
    "- noise_scheduler\n",
    "    - \n",
    "\n",
    "\n",
    "==> Reference: Code extracted from the Imagen implementation by Phil Wang.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Distribution explanation here: https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class GaussianDiffusionContinuousTimes(nn.Module):\n",
    "    def __init__(self, *, noise_schedule, timesteps=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the noise schedule based on the provided argument\n",
    "        if noise_schedule == \"linear\":\n",
    "            self.log_snr = beta_linear_log_snr  # Use linear noise schedule\n",
    "        elif noise_schedule == \"cosine\":\n",
    "            self.log_snr = alpha_cosine_log_snr  # Use cosine noise schedule\n",
    "        else:\n",
    "            raise ValueError(f'invalid noise schedule {noise_schedule}')  # Raise error for invalid schedule\n",
    "\n",
    "        self.num_timesteps = timesteps  # Set the number of timesteps\n",
    "\n",
    "    # Return a tensor of noise levels for each sample in the batch\n",
    "    def get_times(self, batch_size, noise_level, *, device):\n",
    "        return torch.full((batch_size,), noise_level, device=device, dtype=torch.float32)  # Return tensor of noise levels\n",
    "\n",
    "    # Sample random times from a uniform distribution\n",
    "    def sample_random_times(self, batch_size, *, device):\n",
    "        return torch.zeros((batch_size,), device=device).float().uniform_(0, 1)  # Sample random times from a uniform distribution\n",
    "\n",
    "    # Get the condition value based on the times\n",
    "    def get_condition(self, times):\n",
    "        return maybe(self.log_snr)(times)  # Get condition value based on times\n",
    "\n",
    "    # Generate a tensor of sampling timesteps\n",
    "    def get_sampling_timesteps(self, batch, *, device):\n",
    "        times = torch.linspace(1., 0., self.num_timesteps + 1, device=device)  # Generate sampling timesteps\n",
    "        times = repeat(times, 't -> b t', b=batch)  # Repeat timesteps for each sample in the batch\n",
    "        times = torch.stack((times[:, :-1], times[:, 1:]), dim=0)  # Stack timesteps for consecutive samples\n",
    "        times = times.unbind(dim=-1)  # Unbind timesteps along the last dimension\n",
    "        return times  # Return sampling timesteps\n",
    "\n",
    "    # Calculate the posterior distribution parameters\n",
    "    def q_posterior(self, x_start, x_t, t, *, t_next=None):\n",
    "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min=0.))  # Calculate next timestep\n",
    "\n",
    "        log_snr = self.log_snr(t)  # Get log signal-to-noise ratio for current timestep\n",
    "        log_snr_next = self.log_snr(t_next)  # Get log signal-to-noise ratio for next timestep\n",
    "        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))  # Pad dimensions for broadcasting\n",
    "\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)  # Convert log SNR to alpha and sigma\n",
    "        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)  # Convert log SNR of next timestep to alpha and sigma\n",
    "\n",
    "        c = -expm1(log_snr - log_snr_next)  # Calculate c coefficient\n",
    "        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)  # Calculate posterior mean\n",
    "\n",
    "        posterior_variance = (sigma_next ** 2) * c  # Calculate posterior variance\n",
    "        posterior_log_variance_clipped = log(posterior_variance, eps=1e-20)  # Clip and log the posterior variance\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped  # Return posterior parameters\n",
    "\n",
    "    # Sample from the diffusion process\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        dtype = x_start.dtype  # Get the data type of x_start\n",
    "\n",
    "        if isinstance(t, float):  # Check if t is a float\n",
    "            batch = x_start.shape[0]  # Get the batch size\n",
    "            t = torch.full((batch,), t, device=x_start.device, dtype=dtype)  # Create a tensor of t values\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))  # Generate noise if not provided\n",
    "        log_snr = self.log_snr(t).type(dtype)  # Get log SNR and cast to the data type of x_start\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)  # Pad dimensions for broadcasting\n",
    "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)  # Convert log SNR to alpha and sigma\n",
    "\n",
    "        return alpha * x_start + sigma * noise, log_snr, alpha, sigma  # Return sampled value, log SNR, alpha, and sigma\n",
    "\n",
    "    # Sample from the diffusion process from a specified start time to an end time\n",
    "    def q_sample_from_to(self, x_from, from_t, to_t, noise=None):\n",
    "        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype  # Get shape, device, and data type of x_from\n",
    "        batch = shape[0]  # Get the batch size\n",
    "\n",
    "        if isinstance(from_t, float):  # Check if from_t is a float\n",
    "            from_t = torch.full((batch,), from_t, device=device, dtype=dtype)  # Create a tensor of from_t values\n",
    "\n",
    "        if isinstance(to_t, float):  # Check if to_t is a float\n",
    "            to_t = torch.full((batch,), to_t, device=device, dtype=dtype)  # Create a tensor of to_t values\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_from))  # Generate noise if not provided\n",
    "\n",
    "        log_snr = self.log_snr(from_t)  # Get log SNR for from_t\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)  # Pad dimensions for broadcasting\n",
    "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)  # Convert log SNR to alpha and sigma\n",
    "\n",
    "        log_snr_to = self.log_snr(to_t)  # Get log SNR for to_t\n",
    "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)  # Pad dimensions for broadcasting\n",
    "        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)  # Convert log SNR of to_t to alpha and sigma\n",
    "\n",
    "        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha  # Return sampled value\n",
    "\n",
    "    # Predict the start from a given velocity\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        log_snr = self.log_snr(t)  # Get log SNR for t\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)  # Pad dimensions for broadcasting\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)  # Convert log SNR to alpha and sigma\n",
    "        return alpha * x_t - sigma * v  # Return predicted start\n",
    "\n",
    "    # Predict the start from a given noise\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        log_snr = self.log_snr(t)  # Get log SNR for t\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)  # Pad dimensions for broadcasting\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)  # Convert log SNR to alpha and sigma\n",
    "        return (x_t - sigma * noise) / alpha.clamp(min=1e-8)  # Return predicted start\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "842421419649db5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
