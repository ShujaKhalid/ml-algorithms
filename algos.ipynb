{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:31:56.854617Z",
     "start_time": "2024-01-04T18:31:56.206546Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "a = jax.random.uniform(rng, (2,3,3))\n",
    "b = jax.random.uniform(rng, (2,3,4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:36:34.232619Z",
     "start_time": "2024-01-04T18:36:34.169756Z"
    }
   },
   "id": "56fea78d5ef16db4"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.081306\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "einsum examples\n",
    "'''\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Matrix multiplication\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "C = jnp.einsum('ij,jk->ik', A, B)\n",
    "\n",
    "# Element-wise multiplication\n",
    "A = jnp.array([1, 2, 3])\n",
    "B = jnp.array([4, 5, 6])\n",
    "C = jnp.einsum('i,i->i', A, B)\n",
    "\n",
    "# Sum of all elements\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "sum_all = jnp.einsum('ij->', A)\n",
    "\n",
    "# Sum along axis\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "sum_axis0 = jnp.einsum('ij->i', A)\n",
    "sum_axis1 = jnp.einsum('ij->j', A)\n",
    "\n",
    "# Transpose\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "transpose = jnp.einsum('ij->ji', A)\n",
    "\n",
    "# Trace\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "trace = jnp.einsum('ii', A)\n",
    "\n",
    "# Dot product\n",
    "A = jnp.array([1, 2, 3])\n",
    "B = jnp.array([4, 5, 6])\n",
    "dot_product = jnp.einsum('i,i->', A, B)\n",
    "\n",
    "# Batch matrix multiplication\n",
    "A = jnp.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "B = jnp.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n",
    "C = jnp.einsum('ijk,ikl->ijl', A, B)\n",
    "\n",
    "# Outer product\n",
    "A = jnp.array([1, 2, 3])\n",
    "B = jnp.array([4, 5, 6])\n",
    "outer_product = jnp.einsum('i,j->ij', A, B)\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "v = jnp.array([5, 6])\n",
    "C = jnp.einsum('ij,j->i', A, v)\n",
    "\n",
    "# Diagonal of a matrix\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "diagonal = jnp.einsum('ii->i', A)\n",
    "\n",
    "# Kronecker product\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "kron_product = jnp.einsum('ij,kl->ikjl', A, B)\n",
    "\n",
    "# Batch-wise dot product\n",
    "A = jnp.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "B = jnp.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n",
    "batch_dot_product = jnp.einsum('...i,...i', A, B)\n",
    "\n",
    "# Batch-wise matrix multiplication\n",
    "A = jnp.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "B = jnp.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n",
    "batch_matrix_product = jnp.einsum('...ik,...kj->...ij', A, B)\n",
    "\n",
    "# Transpose and matrix multiplication\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "C = jnp.einsum('ij,jk->ki', A, B)\n",
    "\n",
    "# Matrix multiplication and sum along axis\n",
    "A = jnp.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "B = jnp.array([[9, 10], [11, 12]])\n",
    "C = jnp.einsum('ijk,jk->i', A, B)\n",
    "\n",
    "# Trace of batch of matrices\n",
    "A = jnp.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "trace_batch = jnp.einsum('iik->i', A)\n",
    "\n",
    "# Matrix multiplication and sum\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "sum_product = jnp.einsum('ij,jk->', A, B)\n",
    "\n",
    "# Element-wise multiplication and sum\n",
    "A = jnp.array([1, 2, 3])\n",
    "B = jnp.array([4, 5, 6])\n",
    "sum_product_elements = jnp.einsum('i,i->', A, B)\n",
    "\n",
    "# Outer product and sum\n",
    "A = jnp.array([1, 2, 3])\n",
    "B = jnp.array([4, 5, 6])\n",
    "sum_outer_product = jnp.einsum('i,j->', A, B)\n",
    "\n",
    "# Concatenation\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "C = jnp.einsum('ij,ij->ij', A, B)\n",
    "\n",
    "# Matrix-vector multiplication and sum\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "v = jnp.array([5, 6])\n",
    "sum_mv_product = jnp.einsum('ij,j->', A, v)\n",
    "\n",
    "# Tensor contraction\n",
    "A = jnp.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "B = jnp.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n",
    "contracted_tensor = jnp.einsum('ijk,ikl->ijl', A, B)\n",
    "\n",
    "# Element-wise multiplication and sum along axis\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "sum_product_elements_axis0 = jnp.einsum('ij,ij->j', A, B)\n",
    "\n",
    "# Diagonal of matrix product\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "diagonal_product = jnp.einsum('ij,jk->k', A, B)\n",
    "\n",
    "# Kronecker product and sum\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "sum_kron_product = jnp.einsum('ij,kl->ijkl', A, B)\n",
    "\n",
    "# Concatenation and sum along axis\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "sum_concatenation_axis0 = jnp.einsum('ij,ij->i', A, B)\n",
    "\n",
    "# Batch-wise matrix multiplication and sum\n",
    "A = jnp.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "B = jnp.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n",
    "sum_batch_matrix_product = jnp.einsum('...ik,...kj->...ij', A, B)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:41:33.557354Z",
     "start_time": "2024-01-04T18:41:33.551291Z"
    }
   },
   "id": "490e3a0d40c01d07"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nKNN in JAX\\n'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "KNN in JAX\n",
    "'''\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((x1 - x2)**2))\n",
    "\n",
    "def knn(train_X, train_y, test_X, k):\n",
    "    \"\"\"K-Nearest Neighbors algorithm.\"\"\"\n",
    "    distances = jnp.array([[euclidean_distance(train_X[i], test_X[j]) for j in range(len(test_X))] for i in range(len(train_X))])\n",
    "    nearest_indices = jnp.argsort(distances, axis=0)[:k]\n",
    "    nearest_labels = train_y[nearest_indices]\n",
    "    predicted_labels = jnp.array([jnp.argmax(jnp.bincount(nearest_labels[:, i])) for i in range(nearest_labels.shape[1])])\n",
    "    return predicted_labels\n",
    "\n",
    "# Generate synthetic data\n",
    "rng_key = random.PRNGKey(0)\n",
    "train_X = random.normal(rng_key, (100, 2))\n",
    "train_y = random.randint(rng_key, (100,), 0, 2)\n",
    "test_X = random.normal(rng_key, (10, 2))\n",
    "\n",
    "# Perform KNN\n",
    "k = 3\n",
    "predicted_labels = knn(train_X, train_y, test_X, k)\n",
    "\n",
    "print(\"Predicted Labels:\", predicted_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T19:14:16.958718Z",
     "start_time": "2024-01-04T19:14:16.950399Z"
    }
   },
   "id": "ae6d181d3d5d52db"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mNaive Bayes in JAX\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m random\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Naive Bayes in JAX\n",
    "'''\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "def gaussian_pdf(x, mean, std):\n",
    "    return 1.0 / (jnp.sqrt(2 * jnp.pi) * std) * jnp.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "\n",
    "def prior_probabilities(y):\n",
    "    unique, counts = jnp.unique(y, return_counts=True)\n",
    "    return counts / len(y)\n",
    "\n",
    "def fit(X, y):\n",
    "    n_samples, n_features = X.shape\n",
    "    classes = jnp.unique(y)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    prior = prior_probabilities(y)\n",
    "\n",
    "    mean = jnp.zeros((n_classes, n_features))\n",
    "    std = jnp.zeros((n_classes, n_features))\n",
    "\n",
    "    for i, c in enumerate(classes):\n",
    "        X_c = X[y == c]\n",
    "        mean[i] = jnp.mean(X_c, axis=0)\n",
    "        std[i] = jnp.std(X_c, axis=0)\n",
    "\n",
    "    return prior, mean, std\n",
    "\n",
    "def predict(X, prior, mean, std):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = len(prior)\n",
    "    probabilities = jnp.zeros((n_samples, n_classes))\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        probabilities[:, i] = jnp.prod(gaussian_pdf(X, mean[i], std[i]), axis=1) * prior[i]\n",
    "\n",
    "    return jnp.argmax(probabilities, axis=1)\n",
    "\n",
    "# Generate synthetic data\n",
    "rng_key = random.PRNGKey(0)\n",
    "X = random.normal(rng_key, (100, 2))\n",
    "y = jnp.concatenate([jnp.zeros(50), jnp.ones(50)]).astype(jnp.int32)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "prior, mean, std = fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(X_test, prior, mean, std)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = jnp.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T20:57:06.003078Z",
     "start_time": "2024-03-06T20:57:05.820057Z"
    }
   },
   "id": "3e144f2a8124ad82"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2) (2, 2)\n",
      "(5, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PCA in JAX\n",
    "'''\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "# Generate synthetic data\n",
    "data = jnp.array([[1, 2], [2, 3], [8, 9], [9, 10], [5, 6]])\n",
    "\n",
    "# Initialize cluster centroids\n",
    "k = 2\n",
    "centroids = data[:k]\n",
    "\n",
    "# Define the distance function\n",
    "def distance(x, centroids):\n",
    "    return jnp.linalg.norm(x - centroids, axis=1)\n",
    "\n",
    "# Assign data points to the nearest centroids\n",
    "def assign_clusters(data, centroids):\n",
    "    return jnp.argmin(jnp.linalg.norm(data[:, None] - centroids, axis=2), axis=1)\n",
    "\n",
    "# Update centroids to the mean of assigned data points\n",
    "def update_centroids(data, cluster_assignments):\n",
    "    new_centroids = jnp.array([jnp.mean(data[cluster_assignments == i], axis=0) for i in range(k)])\n",
    "    return new_centroids\n",
    "\n",
    "# Perform K-Means clustering\n",
    "for _ in range(100):\n",
    "    cluster_assignments = assign_clusters(data, centroids)\n",
    "    new_centroids = update_centroids(data, cluster_assignments)\n",
    "    \n",
    "    # Check for convergence\n",
    "    if jnp.all(centroids == new_centroids):\n",
    "        break\n",
    "    \n",
    "    centroids = new_centroids\n",
    "\n",
    "# Final cluster assignments\n",
    "print(\"Cluster Assignments:\", cluster_assignments)\n",
    "print(\"Cluster Centroids:\", centroids)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T21:38:01.472561Z",
     "start_time": "2024-01-04T21:38:01.466568Z"
    }
   },
   "id": "9fe130d93811efc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2d72580f32cd1ef8"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9999999 3.9999998 5.9999995 7.9999995]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Linear regression in JAX\n",
    "'''\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# Generate synthetic data\n",
    "X = jnp.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n",
    "y = jnp.array([2, 4, 6, 8])\n",
    "\n",
    "# Initialize model parameters\n",
    "params = jnp.array([0.0, 0.0])\n",
    "\n",
    "# Define the linear regression model\n",
    "def linear_regression(params, x):\n",
    "    return jnp.dot(params, x)\n",
    "\n",
    "# Define the mean squared error loss\n",
    "def mse(params, x, y):\n",
    "    predictions = linear_regression(params, x)\n",
    "    return jnp.mean((predictions - y) ** 2)\n",
    "\n",
    "# Compute gradients and update parameters using gradient descent\n",
    "learning_rate = 0.01\n",
    "for _ in range(100):\n",
    "    gradients = grad(mse)(params, X.T, y)\n",
    "    params -= learning_rate * gradients\n",
    "\n",
    "# Make predictions\n",
    "predictions = linear_regression(params, X.T)\n",
    "print(predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T20:57:26.966581Z",
     "start_time": "2024-01-04T20:57:26.702075Z"
    }
   },
   "id": "87ab774915b172b4"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5671718  0.6211104  0.67221195 0.7624377 ]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Logistic regression in JAX\n",
    "'''\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# Generate synthetic data\n",
    "X = jnp.array([[1, 2], [2, 3], [3, 4], [5, 6]])\n",
    "y = jnp.array([0, 0, 1, 1], dtype=jnp.float32)\n",
    "\n",
    "# Initialize model parameters\n",
    "params = jnp.array([0.0, 0.0])\n",
    "\n",
    "# Define the logistic regression model\n",
    "def logistic_regression(params, x):\n",
    "    z = jnp.dot(params, x)\n",
    "    return 1.0 / (1.0 + jnp.exp(-z))\n",
    "\n",
    "# Define the cross-entropy loss\n",
    "def cross_entropy(params, x, y):\n",
    "    predictions = logistic_regression(params, x)\n",
    "    return -jnp.mean(y * jnp.log(predictions) + (1.0 - y) * jnp.log(1.0 - predictions))\n",
    "\n",
    "# Compute gradients and update parameters using gradient descent\n",
    "learning_rate = 0.01\n",
    "for _ in range(100):\n",
    "    gradients = grad(cross_entropy)(params, X.T, y)\n",
    "    params -= learning_rate * gradients\n",
    "\n",
    "# Make predictions\n",
    "predictions = logistic_regression(params, X.T)\n",
    "print(predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T21:20:54.344138Z",
     "start_time": "2024-01-04T21:20:53.812394Z"
    }
   },
   "id": "d5dab9837d5a38b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + jnp.exp(-x))\n",
    "\n",
    "def neural_network(params, x):\n",
    "    w1, b1, w2, b2 = params\n",
    "    hidden = jnp.tanh(jnp.dot(x, w1) + b1)\n",
    "    output = jnp.dot(hidden, w2) + b2\n",
    "    return output\n",
    "\n",
    "def loss(params, x, y):\n",
    "    output = neural_network(params, x)\n",
    "    return jnp.mean((output - y) ** 2)\n",
    "\n",
    "def init_params(rng_key, input_dim, hidden_dim, output_dim):\n",
    "    rng_key, w1_key, b1_key, w2_key, b2_key = random.split(rng_key, 5)\n",
    "    w1 = random.normal(w1_key, (input_dim, hidden_dim))\n",
    "    b1 = random.normal(b1_key, (hidden_dim,))\n",
    "    w2 = random.normal(w2_key, (hidden_dim, output_dim))\n",
    "    b2 = random.normal(b2_key, (output_dim,))\n",
    "    return (w1, b1, w2, b2)\n",
    "\n",
    "def update(params, x, y, learning_rate):\n",
    "    grad_params = grad(loss)(params, x, y)\n",
    "    return [(param - learning_rate * grad_param)\n",
    "            for param, grad_param in zip(params, grad_params)]\n",
    "\n",
    "# Generate synthetic data\n",
    "rng_key = random.PRNGKey(0)\n",
    "X = random.normal(rng_key, (100, 1))\n",
    "y = 3*X + 1 + random.normal(rng_key, (100, 1)) * 0.1  # y = 3x + 1 + noise\n",
    "\n",
    "# Initialize parameters\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 10\n",
    "output_dim = y.shape[1]\n",
    "params = init_params(rng_key, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Train the model using gradient descent\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    params = update(params, X, y, learning_rate)\n",
    "\n",
    "# Test the model\n",
    "X_test = jnp.array([[1.0]])\n",
    "y_pred = neural_network(params, X_test)\n",
    "print(f\"Predicted y for x=1.0: {y_pred[0][0]}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53e71e0afab4ed6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m2D convolution in JAX\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvolution2d\u001B[39m(image, kernel):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2D convolution in JAX\n",
    "'''\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def convolution2d(image, kernel):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution on an image.\n",
    "\n",
    "    Args:\n",
    "    - image: 2D array representing the input image\n",
    "    - kernel: 2D array representing the convolution kernel\n",
    "\n",
    "    Returns:\n",
    "    - result: 2D array representing the convolved image\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    result_height = image_height - kernel_height + 1\n",
    "    result_width = image_width - kernel_width + 1\n",
    "\n",
    "    result = jnp.zeros((result_height, result_width))\n",
    "\n",
    "    for i in range(result_height):\n",
    "        for j in range(result_width):\n",
    "            result[i, j] = jnp.sum(image[i:i+kernel_height, j:j+kernel_width] * kernel)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Sample 2D image (5x5)\n",
    "image = jnp.array([[1, 2, 3, 4, 5],\n",
    "                    [6, 7, 8, 9, 10],\n",
    "                    [11, 12, 13, 14, 15],\n",
    "                    [16, 17, 18, 19, 20],\n",
    "                    [21, 22, 23, 24, 25]])\n",
    "\n",
    "# Sample kernel (3x3)\n",
    "kernel = jnp.array([[1, 0, -1],\n",
    "                     [1, 0, -1],\n",
    "                     [1, 0, -1]])\n",
    "\n",
    "# Perform convolution\n",
    "result = convolution2d(image, kernel)\n",
    "\n",
    "print(\"Input Image:\")\n",
    "print(image)\n",
    "print(\"\\nConvolution Kernel:\")\n",
    "print(kernel)\n",
    "print(\"\\nConvolved Image:\")\n",
    "print(result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:08:06.320416Z",
     "start_time": "2024-03-06T21:08:06.293254Z"
    }
   },
   "id": "f25bb35aa4434545",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mAdam Optimizer\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grad, jit, random\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madam\u001B[39m(grad, init_params, step_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, b1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m, b2\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.999\u001B[39m, eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-8\u001B[39m, num_iters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Adam Optimizer\n",
    "'''\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "\n",
    "def adam(grad, init_params, step_size=0.001, b1=0.9, b2=0.999, eps=1e-8, num_iters=1000):\n",
    "    \"\"\"\n",
    "    Adam optimizer implementation.\n",
    "\n",
    "    Args:\n",
    "    - grad: Function to compute the gradient of the loss with respect to the parameters.\n",
    "    - init_params: Initial parameters of the model.\n",
    "    - step_size: Step size (learning rate).\n",
    "    - b1: Exponential decay rate for the first moment estimates.\n",
    "    - b2: Exponential decay rate for the second moment estimates.\n",
    "    - eps: Small constant to prevent division by zero.\n",
    "    - num_iters: Number of iterations for optimization.\n",
    "\n",
    "    Returns:\n",
    "    - params: Optimized parameters.\n",
    "    \"\"\"\n",
    "    m = jnp.zeros_like(init_params)\n",
    "    v = jnp.zeros_like(init_params)\n",
    "    params = init_params\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        g = grad(params)\n",
    "        m = b1 * m + (1 - b1) * g\n",
    "        v = b2 * v + (1 - b2) * (g ** 2)\n",
    "        m_hat = m / (1 - b1 ** (i + 1))\n",
    "        v_hat = v / (1 - b2 ** (i + 1))\n",
    "        params = params - step_size * m_hat / (jnp.sqrt(v_hat) + eps)\n",
    "\n",
    "    return params\n",
    "\n",
    "# Test the Adam optimizer on a simple function\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "grad_f = grad(f)\n",
    "init_params = jnp.array(10.0)\n",
    "optimized_params = adam(grad_f, init_params)\n",
    "\n",
    "print(f\"Optimized parameter value: {optimized_params}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:08:56.004235Z",
     "start_time": "2024-03-06T21:08:55.943576Z"
    }
   },
   "id": "d9afadf6ebf6c96a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mLayerNorm in JAX\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlayer_norm\u001B[39m(x, scale, bias, epsilon\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m):\n\u001B[1;32m      7\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m    Layer normalization implementation.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    - Normalized output array of the same shape as x.\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LayerNorm in JAX\n",
    "'''\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def layer_norm(x, scale, bias, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Layer normalization implementation.\n",
    "\n",
    "    Args:\n",
    "    - x: Input array of shape (batch_size, features).\n",
    "    - scale: Scale parameter of shape (features,) for each feature.\n",
    "    - bias: Bias parameter of shape (features,) for each feature.\n",
    "    - epsilon: Small constant to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - Normalized output array of the same shape as x.\n",
    "    \"\"\"\n",
    "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "    var = jnp.var(x, axis=-1, keepdims=True)\n",
    "    x_norm = (x - mean) / jnp.sqrt(var + epsilon)\n",
    "    return scale * x_norm + bias\n",
    "\n",
    "# Test the Layer Normalization implementation\n",
    "batch_size = 4\n",
    "features = 3\n",
    "epsilon = 1e-5\n",
    "scale = jnp.array([1.0, 1.0, 1.0])\n",
    "bias = jnp.array([0.0, 0.0, 0.0])\n",
    "x = jnp.array([[1.0, 2.0, 3.0],\n",
    "                [4.0, 5.0, 6.0],\n",
    "                [7.0, 8.0, 9.0],\n",
    "                [10.0, 11.0, 12.0]])\n",
    "\n",
    "normalized_x = layer_norm(x, scale, bias, epsilon)\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nLayer Normalized Output:\")\n",
    "print(normalized_x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:09:29.191686Z",
     "start_time": "2024-03-06T21:09:29.167760Z"
    }
   },
   "id": "bb365c5de29ce40a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mBatchNorm in JAX\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jit, grad, random\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbatch_norm\u001B[39m(x, scale, bias, epsilon\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BatchNorm in JAX\n",
    "'''\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, random\n",
    "\n",
    "def batch_norm(x, scale, bias, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Batch normalization implementation.\n",
    "\n",
    "    Args:\n",
    "    - x: Input array of shape (batch_size, features).\n",
    "    - scale: Scale parameter of shape (features,) for each feature.\n",
    "    - bias: Bias parameter of shape (features,) for each feature.\n",
    "    - epsilon: Small constant to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - Normalized output array of the same shape as x.\n",
    "    \"\"\"\n",
    "    mean = jnp.mean(x, axis=0, keepdims=True)\n",
    "    var = jnp.var(x, axis=0, keepdims=True)\n",
    "    x_norm = (x - mean) / jnp.sqrt(var + epsilon)\n",
    "    return scale * x_norm + bias\n",
    "\n",
    "# Test the Batch Normalization implementation\n",
    "batch_size = 4\n",
    "features = 3\n",
    "epsilon = 1e-5\n",
    "scale = jnp.array([1.0, 1.0, 1.0])\n",
    "bias = jnp.array([0.0, 0.0, 0.0])\n",
    "x = jnp.array([[1.0, 2.0, 3.0],\n",
    "                [4.0, 5.0, 6.0],\n",
    "                [7.0, 8.0, 9.0],\n",
    "                [10.0, 11.0, 12.0]])\n",
    "\n",
    "normalized_x = batch_norm(x, scale, bias, epsilon)\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nBatch Normalized Output:\")\n",
    "print(normalized_x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:09:57.341320Z",
     "start_time": "2024-03-06T21:09:57.288764Z"
    }
   },
   "id": "c348e38ffe21feaa",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mSuper resolution code in JAX\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grad, jit, random\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Super resolution code in JAX\n",
    "'''\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "def init_params(rng, layer_sizes):\n",
    "    \"\"\"Initialize model parameters.\"\"\"\n",
    "    scale = 1.0 / jnp.sqrt(layer_sizes[0])\n",
    "    return [(random.normal(rng, (m, n)) * scale, jnp.zeros(n))\n",
    "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def upsampling_layer(params, x):\n",
    "    \"\"\"Custom upsampling layer using transposed convolution.\"\"\"\n",
    "    w, b = params\n",
    "    output_shape = (x.shape[0], x.shape[1]*2, x.shape[2]*2, w.shape[1])\n",
    "    return jnp.conv_transpose(x, w, (2, 2), 'VALID') + b\n",
    "\n",
    "def super_resolution_model(params, x):\n",
    "    \"\"\"Super-resolution model architecture.\"\"\"\n",
    "    h1 = relu(upsampling_layer(params[0], x))\n",
    "    return h1\n",
    "\n",
    "# Generate synthetic low-resolution image\n",
    "rng_key = random.PRNGKey(0)\n",
    "low_res_image = random.normal(rng_key, (1, 16, 16, 3))\n",
    "\n",
    "# Initialize model parameters\n",
    "layer_sizes = [48, 3*2*2*3]  # Custom upsampling layer with 2x2 kernel\n",
    "params = init_params(rng_key, layer_sizes)\n",
    "\n",
    "# Upsample the low-resolution image\n",
    "upsampled_image = super_resolution_model(params, low_res_image)\n",
    "\n",
    "print(\"Low-resolution image shape:\", low_res_image.shape)\n",
    "print(\"Upsampled image shape:\", upsampled_image.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:11:28.570317Z",
     "start_time": "2024-03-06T21:11:28.504516Z"
    }
   },
   "id": "20da83601ed9e88",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mLSTM in JAX\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m random\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LSTM in JAX\n",
    "'''\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + jnp.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "def lstm_cell(prev_c, prev_h, x, params):\n",
    "    \"\"\"LSTM cell implementation.\"\"\"\n",
    "    W_i, b_i, W_f, b_f, W_o, b_o, W_c, b_c = params\n",
    "\n",
    "    # Input gate\n",
    "    i = sigmoid(jnp.dot(x, W_i) + jnp.dot(prev_h, W_i) + b_i)\n",
    "\n",
    "    # Forget gate\n",
    "    f = sigmoid(jnp.dot(x, W_f) + jnp.dot(prev_h, W_f) + b_f)\n",
    "\n",
    "    # Output gate\n",
    "    o = sigmoid(jnp.dot(x, W_o) + jnp.dot(prev_h, W_o) + b_o)\n",
    "\n",
    "    # Candidate memory cell update\n",
    "    c_tilde = tanh(jnp.dot(x, W_c) + jnp.dot(prev_h, W_c) + b_c)\n",
    "\n",
    "    # Cell state update\n",
    "    c = f * prev_c + i * c_tilde\n",
    "\n",
    "    # Hidden state computation\n",
    "    h = o * tanh(c)\n",
    "\n",
    "    return c, h\n",
    "\n",
    "# Initialize LSTM cell parameters\n",
    "def init_lstm_params(rng_key, input_size, hidden_size):\n",
    "    \"\"\"Initialize LSTM cell parameters.\"\"\"\n",
    "    rng_key, W_i_key, b_i_key, W_f_key, b_f_key, W_o_key, b_o_key, W_c_key, b_c_key = random.split(rng_key, 9)\n",
    "    W_i = random.normal(W_i_key, (input_size, hidden_size))\n",
    "    b_i = jnp.zeros(hidden_size)\n",
    "    W_f = random.normal(W_f_key, (input_size, hidden_size))\n",
    "    b_f = jnp.zeros(hidden_size)\n",
    "    W_o = random.normal(W_o_key, (input_size, hidden_size))\n",
    "    b_o = jnp.zeros(hidden_size)\n",
    "    W_c = random.normal(W_c_key, (input_size, hidden_size))\n",
    "    b_c = jnp.zeros(hidden_size)\n",
    "    return (W_i, b_i, W_f, b_f, W_o, b_o, W_c, b_c)\n",
    "\n",
    "# Test the LSTM cell\n",
    "input_size = 10\n",
    "hidden_size = 5\n",
    "rng_key = random.PRNGKey(0)\n",
    "x = random.normal(rng_key, (1, input_size))\n",
    "prev_c = jnp.zeros((1, hidden_size))\n",
    "prev_h = jnp.zeros((1, hidden_size))\n",
    "params = init_lstm_params(rng_key, input_size, hidden_size)\n",
    "next_c, next_h = lstm_cell(prev_c, prev_h, x, params)\n",
    "\n",
    "print(\"Input size:\", input_size)\n",
    "print(\"Hidden size:\", hidden_size)\n",
    "print(\"Input x shape:\", x.shape)\n",
    "print(\"Output c shape:\", next_c.shape)\n",
    "print(\"Output h shape:\", next_h.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:26:37.133542Z",
     "start_time": "2024-03-06T21:26:37.091116Z"
    }
   },
   "id": "1d617ceb009788ca",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "Various distances\n",
    "'''\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return jnp.sqrt(jnp.sum((x1 - x2) ** 2))\n",
    "\n",
    "def squared_euclidean_distance(x1, x2):\n",
    "    return jnp.sum((x1 - x2) ** 2)\n",
    "\n",
    "def manhattan_distance(x1, x2):\n",
    "    return jnp.sum(jnp.abs(x1 - x2))\n",
    "\n",
    "def chebyshev_distance(x1, x2):\n",
    "    return jnp.max(jnp.abs(x1 - x2))\n",
    "\n",
    "def cosine_similarity(x1, x2):\n",
    "    dot_product = jnp.dot(x1, x2)\n",
    "    norm_x1 = jnp.sqrt(jnp.sum(x1 ** 2))\n",
    "    norm_x2 = jnp.sqrt(jnp.sum(x2 ** 2))\n",
    "    return dot_product / (norm_x1 * norm_x2 + 1e-8)\n",
    "\n",
    "def minkowski_distance(x1, x2, p):\n",
    "    return jnp.sum(jnp.abs(x1 - x2) ** p) ** (1.0 / p)\n",
    "\n",
    "def hamming_distance(x1, x2):\n",
    "    return jnp.sum(x1 != x2)\n",
    "\n",
    "def jaccard_distance(x1, x2):\n",
    "    intersection = jnp.sum(x1 & x2)\n",
    "    union = jnp.sum(x1 | x2)\n",
    "    return 1.0 - intersection / (union + 1e-8)\n",
    "\n",
    "def dice_similarity(x1, x2):\n",
    "    intersection = jnp.sum(x1 & x2)\n",
    "    dice_coefficient = 2.0 * intersection / (jnp.sum(x1) + jnp.sum(x2))\n",
    "    return 1.0 - dice_coefficient\n",
    "\n",
    "def kullback_leibler_divergence(p, q):\n",
    "    return jnp.sum(p * jnp.log(p / q + 1e-8))\n",
    "\n",
    "# Example usage\n",
    "x1 = jnp.array([1, 2, 3])\n",
    "x2 = jnp.array([4, 5, 6])\n",
    "\n",
    "print(\"Euclidean Distance:\", euclidean_distance(x1, x2))\n",
    "print(\"Squared Euclidean Distance:\", squared_euclidean_distance(x1, x2))\n",
    "print(\"Manhattan Distance:\", manhattan_distance(x1, x2))\n",
    "print(\"Chebyshev Distance:\", chebyshev_distance(x1, x2))\n",
    "print(\"Cosine Similarity:\", cosine_similarity(x1, x2))\n",
    "print(\"Minkowski Distance (p=3):\", minkowski_distance(x1, x2, 3))\n",
    "print(\"Hamming Distance:\", hamming_distance(x1, x2))\n",
    "print(\"Jaccard Distance:\", jaccard_distance(x1 > 2, x2 > 5))\n",
    "print(\"Dice Similarity:\", dice_similarity(x1 > 2, x2 > 5))\n",
    "print(\"Kullback-Leibler Divergence:\", kullback_leibler_divergence(x1, x2))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "638f4e24838e183a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mVarious evaluation metrics in JAX\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Classification Metrics\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maccuracy\u001B[39m(y_true, y_pred):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Various evaluation metrics in JAX\n",
    "'''\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Classification Metrics\n",
    "def accuracy(y_true, y_pred):\n",
    "    return jnp.mean(y_true == y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = jnp.sum((y_true == 1) & (y_pred == 1))\n",
    "    predicted_positives = jnp.sum(y_pred == 1)\n",
    "    return true_positives / (predicted_positives + 1e-8)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = jnp.sum((y_true == 1) & (y_pred == 1))\n",
    "    actual_positives = jnp.sum(y_true == 1)\n",
    "    return true_positives / (actual_positives + 1e-8)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-8)\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    return jnp.array([[jnp.sum((y_true == 1) & (y_pred == 1)), jnp.sum((y_true == 0) & (y_pred == 1))],\n",
    "                       [jnp.sum((y_true == 1) & (y_pred == 0)), jnp.sum((y_true == 0) & (y_pred == 0))]])\n",
    "\n",
    "# Regression Metrics\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return jnp.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return jnp.mean(jnp.abs(y_true - y_pred))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = jnp.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = jnp.sum((y_true - jnp.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / (ss_tot + 1e-8))\n",
    "\n",
    "# NLP Metrics\n",
    "def bleu_score(reference_corpus, candidate_corpus):\n",
    "    raise NotImplementedError(\"BLEU score calculation is not implemented in JAX.\")\n",
    "\n",
    "def rouge_score(reference_corpus, candidate_corpus):\n",
    "    raise NotImplementedError(\"ROUGE score calculation is not implemented in JAX.\")\n",
    "\n",
    "# Other Metrics\n",
    "def mean_iou(y_true, y_pred):\n",
    "    intersection = jnp.sum(y_true & y_pred)\n",
    "    union = jnp.sum(y_true | y_pred)\n",
    "    return intersection / (union + 1e-8)\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    intersection = jnp.sum(y_true & y_pred)\n",
    "    return 2 * intersection / (jnp.sum(y_true) + jnp.sum(y_pred) + 1e-8)\n",
    "\n",
    "# Example usage\n",
    "y_true = jnp.array([1, 0, 1, 1, 0])\n",
    "y_pred = jnp.array([1, 1, 0, 1, 0])\n",
    "\n",
    "print(\"Accuracy:\", accuracy(y_true, y_pred))\n",
    "print(\"Precision:\", precision(y_true, y_pred))\n",
    "print(\"Recall:\", recall(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_true, y_pred))\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_true, y_pred))\n",
    "print(\"R Squared:\", r_squared(y_true, y_pred))\n",
    "print(\"Mean IoU:\", mean_iou(y_true, y_pred))\n",
    "print(\"Dice Coefficient:\", dice_coefficient(y_true, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T21:39:31.893813Z",
     "start_time": "2024-03-06T21:39:31.810822Z"
    }
   },
   "id": "d5fc8f51b08047c2",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "mAP in JAX\n",
    "'''\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU (Intersection over Union) between two bounding boxes.\"\"\"\n",
    "    x1_tl, y1_tl, x1_br, y1_br = box1\n",
    "    x2_tl, y2_tl, x2_br, y2_br = box2\n",
    "\n",
    "    intersection_width = jnp.maximum(0, jnp.minimum(x1_br, x2_br) - jnp.maximum(x1_tl, x2_tl))\n",
    "    intersection_height = jnp.maximum(0, jnp.minimum(y1_br, y2_br) - jnp.maximum(y1_tl, y2_tl))\n",
    "    intersection_area = intersection_width * intersection_height\n",
    "\n",
    "    box1_area = (x1_br - x1_tl) * (y1_br - y1_tl)\n",
    "    box2_area = (x2_br - x2_tl) * (y2_br - y2_tl)\n",
    "\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    return intersection_area / (union_area + 1e-8)\n",
    "\n",
    "def calculate_precision_recall(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Calculate precision and recall for a given IoU threshold.\"\"\"\n",
    "    num_pred_boxes = len(pred_boxes)\n",
    "    num_gt_boxes = len(gt_boxes)\n",
    "    tp = jnp.zeros(num_pred_boxes)\n",
    "    fp = jnp.zeros(num_pred_boxes)\n",
    "    fn = jnp.zeros(num_gt_boxes)\n",
    "\n",
    "    for i, pred_box in enumerate(pred_boxes):\n",
    "        ious = [calculate_iou(pred_box, gt_box) for gt_box in gt_boxes]\n",
    "        max_iou_idx = jnp.argmax(ious)\n",
    "        if ious[max_iou_idx] >= iou_threshold:\n",
    "            if not fn[max_iou_idx]:\n",
    "                tp[i] = 1\n",
    "            else:\n",
    "                fp[i] = 1\n",
    "        else:\n",
    "            fp[i] = 1\n",
    "\n",
    "    tp_cumsum = jnp.cumsum(tp)\n",
    "    fp_cumsum = jnp.cumsum(fp)\n",
    "    fn_cumsum = jnp.cumsum(fn)\n",
    "\n",
    "    precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)\n",
    "    recall = tp_cumsum / (tp_cumsum + fn_cumsum + 1e-8)\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_average_precision(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Calculate average precision (AP) for a given IoU threshold.\"\"\"\n",
    "    precision, recall = calculate_precision_recall(gt_boxes, pred_boxes, iou_threshold)\n",
    "\n",
    "    # Compute the precision-recall curve\n",
    "    sorted_indices = jnp.argsort(recall)\n",
    "    precision = precision[sorted_indices]\n",
    "    recall = recall[sorted_indices]\n",
    "\n",
    "    # Calculate the area under the precision-recall curve\n",
    "    ap = jnp.trapz(precision, recall)\n",
    "\n",
    "    return ap\n",
    "\n",
    "def calculate_mAP(gt_boxes_list, pred_boxes_list, iou_threshold=0.5):\n",
    "    \"\"\"Calculate mean Average Precision (mAP) across all classes.\"\"\"\n",
    "    num_classes = len(gt_boxes_list)\n",
    "    aps = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        ap = calculate_average_precision(gt_boxes_list[i], pred_boxes_list[i], iou_threshold)\n",
    "        aps.append(ap)\n",
    "\n",
    "    mAP = jnp.mean(aps)\n",
    "    return mAP\n",
    "\n",
    "# Example usage\n",
    "gt_boxes_list = [[(0, 0, 1, 1), (0.5, 0.5, 1.5, 1.5)], [(1, 1, 2, 2)], [(0, 0, 1, 1)]]\n",
    "pred_boxes_list = [[(0, 0, 1, 1), (0.5, 0.5, 1.5, 1.5)], [(1, 1, 2, 2)], [(0, 0, 1, 1)]]\n",
    "\n",
    "mAP = calculate_mAP(gt_boxes_list, pred_boxes_list, iou_threshold=0.5)\n",
    "print(\"mAP:\", mAP)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "376070243b700ee1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
